{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593cd5fe",
   "metadata": {},
   "source": [
    "**Importar bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac39b010-112f-4b57-af79-b70a7af97307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, ConvLSTM1D\n",
    "from keras.models import Sequential, Model\n",
    "X = np.load('X.npy')\n",
    "Y = np.load('Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706ce55",
   "metadata": {},
   "source": [
    "**Contrucción de la arquitectura de la red neuronal**     \n",
    "Hemos deicidido partir de crear una implementación de un bloque de la arquitectura transformer parametrizable, para poder crear una red neuronal con un número de bloques y cabezas de atención variable. Este hiperparámetro luego se ajustaron con ayuda de keras tuner. Aunque solo incluiremos la arquitectura final, para evitar redundancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd35f7a-aa99-4595-b975-92c9e1741ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Función para construir el bloque transformer\n",
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    attn_output = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    attn_output = Dropout(dropout)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + inputs)\n",
    "    \n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = Dropout(dropout)(ffn_output)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(ffn_output + out1)\n",
    "    return out2\n",
    "\n",
    "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_blocks, num_classes, dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_block(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Parámetros del modelo\n",
    "input_shape = (15, 768)\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 448\n",
    "num_blocks = 5\n",
    "num_classes = 3\n",
    "dropout = 0.4\n",
    "\n",
    "model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_blocks, num_classes, dropout)\n",
    "\n",
    "# Compilar el modelo\n",
    "optimizer = Adam(learning_rate=0.00007)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317b25a",
   "metadata": {},
   "source": [
    "**Parte de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e94bf-fc91-4b08-aac8-03c9d2288851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=115, stratify=Y)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=45, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=300,  # Número de épocas, ajusta según sea necesario\n",
    "    batch_size=71,  # Tamaño del batch, ajusta según sea necesario\n",
    "    validation_data=(X_val, Y_val),\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5aebea",
   "metadata": {},
   "source": [
    "**Parte de desglose de validación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "595ea6e7-0fce-4489-a206-1262353b4661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98        74\n",
      "           1       0.96      0.97      0.96       323\n",
      "           2       0.89      0.84      0.86        74\n",
      "\n",
      "    accuracy                           0.95       471\n",
      "   macro avg       0.94      0.93      0.94       471\n",
      "weighted avg       0.95      0.95      0.95       471\n",
      "\n",
      "[[ 73   1   0]\n",
      " [  2 313   8]\n",
      " [  0  12  62]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "print(classification_report(Y_val, Y_pred))\n",
    "print(confusion_matrix(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e017a-8254-4705-a0f0-3742630b3643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Precisión del modelo')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Pérdida del modelo')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
    "    plt.show() \n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34461ca2",
   "metadata": {},
   "source": [
    "**Finalmente que estuvimos satisfechos con el modelos y habiamos decidido ya no hacer más ajustes, se procedió a evaluar el rendimiento del modelo en el conjunto de prueba.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa16352a-923f-4bbf-9fbc-7c0999ef9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = tf.keras.models.load_model('best_modelTT.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59043b7d-5cdb-4e17-91d7-e594daed85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X-test.npy')\n",
    "Y = np.load('Y-test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495edfc-c1ba-4d09-afe9-6352ef2ec10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = np.argmax(model.predict(X), axis=1)\n",
    "print(classification_report(Y, Y_pred))\n",
    "print(confusion_matrix(Y, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3903b6-9d76-4b68-b9e4-c37e5877b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = np.argmax(model.predict(X), axis=1)\n",
    "print(classification_report(Y, Y_pred))\n",
    "print(confusion_matrix(Y, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
